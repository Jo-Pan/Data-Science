{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Packages for loading, cleaning, visualization, and analysis\n",
    "\n",
    "# Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import os\n",
    "import string as st\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Class Exercise  -  Save the Cleaned Data Frame\n",
    "\n",
    "Copy the cells from your Data Engineering 3 - Data Cleaning notebook to this notebook. Only copy the cells that you use to clean the data (i.e., if you use imputation rather than row deletion copy the imputation cells and not the row deletion cells). \n",
    "\n",
    "Run the code in the cells and then save your results, the cleaned data frame, to your local or cloud storage. \n",
    "\n",
    "Read the data to validate that you correctly saved your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to get the files from a directory\n",
    "\n",
    "def getallfiles(directory, extension = \".txt\"):\n",
    "    '''Get all files in directory with the specified extension\n",
    "        and put them into a list.\n",
    "        The default extension is txt. The directory parameter is the path to \n",
    "        the directory containing the files.'''\n",
    "    filenames = os.listdir(directory)\n",
    "    myfiles = []\n",
    "    for e in filenames:\n",
    "        if e.endswith(extension):\n",
    "            myfiles.append(os.path.realpath(e))\n",
    "    return myfiles\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createlist(directory, extension = \".txt\"):\n",
    "    '''Put all files in the specified directory\n",
    "    with the chosen extension (txt is the default) \n",
    "    into a datafame'''\n",
    "    os.chdir(directory)\n",
    "    files = getallfiles(directory)\n",
    "    filelist = []\n",
    "    for i,file in enumerate(files):\n",
    "        filelist.append(pd.read_csv(os.path.realpath(file), low_memory = False, encoding = \"ISO-8859-1\"))\n",
    "    return(filelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51623, 153)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the data frame of all accidents\n",
    "\n",
    "path = \"/Users/Pan/Google Drive/Data Science/DS6001\"\n",
    "acts = createlist(path)\n",
    "accidents_df = pd.concat(acts,ignore_index=True)\n",
    "accidents_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Join the narrative and put them in a list\n",
    "\n",
    "def join_narratives(DF):\n",
    "    '''With the input of the accident dataframe\n",
    "    merge the narrative columns into a single narrative\n",
    "    and return a list of these single narratives for each\n",
    "    accident report in the dataframe. '''\n",
    "    narrlist = []\n",
    "    for i in range(0,15):\n",
    "        a = str(i+1)\n",
    "        narrlist.append('NARR'+ a)\n",
    "    RailNarr = DF.loc[:, narrlist]\n",
    "    Narratives = []\n",
    "    for i, _ in enumerate(RailNarr[\"NARR1\"]):\n",
    "        NarrativeList = RailNarr.iloc[i]\n",
    "        Anarrative = \"\"\n",
    "        for narr in NarrativeList:\n",
    "            if pd.isnull(narr):\n",
    "                break\n",
    "            else:\n",
    "                Anarrative += str(narr)\n",
    "        Narratives.append(Anarrative)\n",
    "    return (Narratives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LUS31-03 ATTEMPTING TO DEPART YARD IN OGDEN WITH 164 CARS AND 16,000 TONS ON A 13 DEGREE CURVE.  ENGINEER NOTCHED UP PER THE SEQUENCE AND WENT TO NOTCH 6 AND HELD AT NOTCH 6 FOR 12 SECONDS BUT WAS UNABLE TO MOVE THE TRAIN.  UPON INSPECTION DISCOVERED THAT 10 CARS HAD STRING LINED AND DERAILED.'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "narrative_list = join_narratives(accidents_df)\n",
    "accidents_df[\"Narrative\"] = narrative_list\n",
    "\n",
    "#Check by looking at narrative 3\n",
    "accidents_df[\"Narrative\"][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Drop the old narrative columns\n",
    "narrlist = []\n",
    "for i in range(0,15):\n",
    "    a = str(i+1)\n",
    "    narrlist.append('NARR'+a)\n",
    "accidents_df.drop(narrlist,axis =1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30677, 139)\n",
      "(35453, 139)\n",
      "(38167, 139)\n"
     ]
    }
   ],
   "source": [
    "# Look at 3 approaches to removing duplicates\n",
    "\n",
    "#Removing duplicates\n",
    "accidents_clean_df = accidents_df.drop_duplicates(['YEAR','DAY','MONTH','TIMEHR'])\n",
    "print(accidents_clean_df.shape)\n",
    "\n",
    "#Using AMPM\n",
    "accidents_clean_df = accidents_df.drop_duplicates(['YEAR','DAY','MONTH','TIMEHR','AMPM'])\n",
    "print(accidents_clean_df.shape)\n",
    "\n",
    "# Using the FRA FAQ (look at the source of the data)\n",
    "accidents_clean_df = accidents_df[(accidents_df['JOINTCD']==1)&(accidents_df['TYPE']!=7)]\n",
    "print(accidents_clean_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accidents_clean_df = accidents_clean_df.dropna(axis=1,thresh = (38167-1500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "class DataFrameImputer(TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Impute missing values.\n",
    "\n",
    "        Columns of dtype object are imputed with the most frequent value \n",
    "        in column.\n",
    "        \n",
    "        Columns of dtype floating point are imputed with the mean.\n",
    "\n",
    "        Columns of other types are imputed with median of the column.\n",
    "\n",
    "        \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        self.fill = pd.Series([X[c].value_counts().index[0]\n",
    "            if X[c].dtype == np.dtype('O') \n",
    "                               else X[c].mean() if X[c].dtype == np.dtype('f')\n",
    "                                else X[c].median() for c in X],\n",
    "            index=X.columns)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X.fillna(self.fill)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accidents_clean_df = DataFrameImputer().fit_transform(accidents_clean_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38167, 88)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accidents_clean_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Defailment      27429\n",
       "Other            4321\n",
       "SeeNarrative     2127\n",
       "Side             1603\n",
       "Obstruction       991\n",
       "Raking            720\n",
       "Fire              454\n",
       "Rearend           298\n",
       "Headon            125\n",
       "BrokenTrain        80\n",
       "Explosive          14\n",
       "GradeX              5\n",
       "Name: TYPE, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4. Replace values TYPE\n",
    "accidents_clean_df['TYPE']=accidents_clean_df['TYPE'].replace(range(1,14),\n",
    "                                    [\"Defailment\",\"Headon\",\"Rearend\",\"Side\",\"Raking\",\n",
    "                                    \"BrokenTrain\",\"Hwy-Rail\",\"GradeX\",\"Obstruction\",\n",
    "                                    \"Explosive\",\"Fire\",\"Other\",\"SeeNarrative\"])\n",
    "accidents_clean_df['TYPE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Freight          17988\n",
       "Yard             12910\n",
       "CutofCars         1950\n",
       "Light             1719\n",
       "PassengerPull     1188\n",
       "Single            1048\n",
       "Commuter           363\n",
       "Maint of Way       311\n",
       "Maint              254\n",
       "Work               252\n",
       "EMU                101\n",
       "CommuterPush        38\n",
       "PassengerPush       28\n",
       "DMU                 17\n",
       "Name: TYPEQ, dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map for TYPEQ\n",
    "# Taken from Rail Equipment Accident/ Incident Report Form\n",
    "\n",
    "map_typeq = {1:\"Freight\",2:\"PassengerPull\",3:\"Commuter\",4:\"Work\",5:\"Single\",\n",
    "            6:\"CutofCars\",7:\"Yard\",8:\"Light\",9:\"Maint\",'A':\"Maint of Way\",\n",
    "            '1':\"Freight\",'2':\"PassengerPull\",'3':'Commuter','4':\"Work\",'5':\"Single\",\n",
    "            '6':\"CutofCars\",'7':\"Yard\",'8':\"Light\",'9':\"Maint\",'B':\"PassengerPush\",'C':\"CommuterPush\",\n",
    "            'D':\"EMU\",'E':\"DMU\"}\n",
    "accidents_clean_df['TYPEQ'] = accidents_clean_df['TYPEQ'].map(map_typeq)\n",
    "accidents_clean_df['TYPEQ'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Narrative mess it up,since it has comma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON intro: www.json.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'/Users/Pan/Google Drive/Data Science/DS6001/TrainAccidents_Clean.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-5fbc9696165c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/Users/Pan/Google Drive/Data Science/DS6001/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"TrainAccidents_Clean.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0maccidents_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0maccidents_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Pan/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Pan/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Pan/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    762\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Pan/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    983\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Pan/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1605\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas/_libs/parsers.c:4209)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas/_libs/parsers.c:8873)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'/Users/Pan/Google Drive/Data Science/DS6001/TrainAccidents_Clean.csv' does not exist"
     ]
    }
   ],
   "source": [
    "path = \"/Users/Pan/Google Drive/Data Science/DS6001/\"\n",
    "file=\"TrainAccidents_Clean.csv\"\n",
    "accidents_df=pd.read_csv(path+file,low_memory=False)\n",
    "accidents_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#now save the Narrative\n",
    "#first create the dictionary of narratives\n",
    "#note: JSON will not allow integer values for indices\n",
    "\n",
    "str_index=[str(x) for x in accidents_clean_df.index]\n",
    "\n",
    "Narrative_dict=dict(zip(str_index,accidents_clean_df.Narrative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "path = \"/Users/Pan/Google Drive/Data Science/DS6001/\"\n",
    "file=\"TrainAccidents_Clean.txt\"\n",
    "with open(path+file,\"w\")as destination:\n",
    "    json.dump(Narrative_dict,destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Y-BIR3041-06 WHILE PULLING 29 CARS FROM 4101 TRACK DERAILED 2 CARS DUE TO SOFT ROADED. CUT CONTAINEDARTICULATED EQUIPMENT.',\n",
       " 'UECDAD-01 PULLING EMPTY BOX CARS FROM STORAGE, COUPLED TO FIRST CARS AND PROCEEDED TO MAKE 3 JOINTSSHOVING BACK AND STRETCH ALL JOINTS MADE.  AFTER COUPLING ALL THE CARS, TRAIN STARTED TO MOVE NORTHAND AIR WENT INTO EMERGENCY AND WOULD NOT RECOVER.  INSPECTION REVEALED 6 CARS DERAILED.  INVESTIGATION REVEALED A 65 FOOT TREE HAD FALLEN AGAINST THE WEST SIDE OF THE TRAIN AND AS THE CARS MOVED THETREE TOP FELL IN BETWEEN THE CARS CAUSING THEM TO DERAIL.']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read the JSON file\n",
    "#look to see if it is correct\n",
    "\n",
    "with open (path+file) as json_file:\n",
    "\n",
    "    Narrative_dict=json.load(json_file)\n",
    "\n",
    "str_index = [int(x) for x in Narrative_dict.keys()]\n",
    "Narrative_dict=dict(zip(str_index,Narrative_dict.values()))\n",
    "[Narrative_dict.get(key) for key in(0,1)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file=\"TrainAccidents_Clean.csv\"\n",
    "accidents_noNarrative_df = accidents_clean_df.drop(\"Narrative\",axis=1)\n",
    "accidents_noNarrative_df.to_csv(path+file,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38167, 87)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the data to see if it is the same\n",
    "file=\"TrainAccidents_Clean.csv\"\n",
    "accidents_df=pd.read_csv(path+file,low_memory=False)\n",
    "accidents_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38167, 88)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accidents_df['Narrative']=Narrative_dict.values()\n",
    "accidents_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
